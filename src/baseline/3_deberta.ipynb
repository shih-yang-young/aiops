{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e76aa66-729a-4bf9-98b9-61d8114e9f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inbound Message</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>login issue verified user details employee man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>outlook hello team meetings skype meetings etc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cant log vpn cannot log vpn best</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unable access tool page unable access tool page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skype error skype error</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449</th>\n",
       "      <td>emails coming mail good afternoon receiving em...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>telephony software issue telephony software issue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8451</th>\n",
       "      <td>vip windows password reset tifpdchb pedxruyf v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452</th>\n",
       "      <td>machine est funcionando unable access machine ...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8453</th>\n",
       "      <td>mehreren lassen sich verschiedene prgramdntyme...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8454 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Inbound Message  Label\n",
       "0     login issue verified user details employee man...      0\n",
       "1     outlook hello team meetings skype meetings etc...      0\n",
       "2                      cant log vpn cannot log vpn best      0\n",
       "3       unable access tool page unable access tool page      0\n",
       "4                               skype error skype error      0\n",
       "...                                                 ...    ...\n",
       "8449  emails coming mail good afternoon receiving em...     22\n",
       "8450  telephony software issue telephony software issue      0\n",
       "8451  vip windows password reset tifpdchb pedxruyf v...      0\n",
       "8452  machine est funcionando unable access machine ...     44\n",
       "8453  mehreren lassen sich verschiedene prgramdntyme...     49\n",
       "\n",
       "[8454 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_path = \"../../data/open_source_8454_combine_short_description.csv\"  # 替換為你的檔案路徑\n",
    "df = pd.read_csv(data_path)\n",
    "# df_label_0 = df[df[\"Label\"] == 0]\n",
    "# df_label_0_to_drop = df_label_0.sample(n=3000, random_state=42)\n",
    "# df = df.drop(df_label_0_to_drop.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79be804-d1e3-463a-824f-d4765544e493",
   "metadata": {},
   "source": [
    "# 十折驗證法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6a5029-3898-4bab-8d6c-bfd1a3694153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Available: True\n",
      "CUDA Device: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Current GPU Memory Allocated: 0.0 GB\n",
      "Current GPU Memory Cached: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\") \n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Current GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9} GB\")\n",
    "print(f\"Current GPU Memory Cached: {torch.cuda.memory_reserved() / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833a0227-7775-4659-a98c-48c3fbdc9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, messages, labels, tokenizer, max_length):\n",
    "        self.messages = messages\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.messages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        message = str(self.messages[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            message,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985bf187-a1b5-4882-9e52-2cbc0188e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義訓練與評估函數\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dceb8a1-edf1-42fe-ac48-dc84ed4709cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def eval_model(model, dataloader, criterion, num_labels):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    val_loss = total_loss / len(dataloader)\n",
    "    val_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": f1,\n",
    "        \"macro_f1-score\": macro_f1,\n",
    "        \"balanced_accuracy\": balanced_acc,\n",
    "        \"mcc\": mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0217a72c-db4d-4875-8a00-33f2dae9de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "from sklearn.model_selection import KFold\n",
    "# 設定參數\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "# 初始化 BERT 的 tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "\n",
    "# 初始化數據\n",
    "messages = df['Inbound Message'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "\n",
    "# KFold 初始化\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee99c1b8-2fc7-483a-8cb6-d375039c6cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Training set label distribution: {0: 3585, 1: 28, 2: 123, 3: 26, 4: 239, 5: 134, 6: 101, 7: 37, 8: 80, 9: 70, 10: 82, 11: 199, 12: 209, 13: 34, 14: 25, 15: 31, 16: 22, 17: 258, 18: 106, 19: 47, 20: 15, 21: 39, 22: 89, 23: 183, 24: 15, 25: 55, 26: 100, 27: 52, 28: 13, 29: 15, 30: 17, 31: 89, 32: 39, 33: 36, 34: 29, 35: 13, 36: 32, 37: 24, 38: 5, 39: 118, 40: 10, 41: 10, 42: 166, 43: 18, 44: 23, 45: 11, 46: 58, 47: 601, 48: 215, 49: 82}\n",
      "Validation set label distribution: {0: 389, 1: 3, 2: 17, 3: 4, 4: 18, 5: 11, 6: 17, 7: 2, 8: 5, 9: 11, 10: 6, 11: 16, 12: 32, 13: 2, 14: 4, 16: 3, 17: 31, 18: 10, 19: 9, 20: 3, 21: 5, 22: 8, 23: 17, 24: 5, 25: 8, 26: 7, 27: 9, 28: 2, 29: 1, 30: 2, 31: 11, 32: 6, 33: 4, 34: 8, 35: 2, 36: 3, 37: 3, 38: 2, 39: 11, 40: 4, 41: 1, 42: 18, 43: 2, 44: 2, 46: 10, 47: 60, 48: 37, 49: 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/476 [00:13<35:36,  4.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_dataloader, optimizer, criterion)\n\u001b[0;32m     42\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m eval_model(model, val_dataloader, criterion, num_labels\u001b[38;5;241m=\u001b[39mNUM_LABELS)\n\u001b[0;32m     43\u001b[0m     epoch_results\u001b[38;5;241m.\u001b[39mappend(metrics)\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "# 進行 10 折交叉驗證\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(messages)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    train_messages = [messages[i] for i in train_idx]\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_messages = [messages[i] for i in val_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    print(\"Training set label distribution:\", {label: train_labels.count(label) for label in set(train_labels)})\n",
    "    print(\"Validation set label distribution:\", {label: val_labels.count(label) for label in set(val_labels)})\n",
    "    \n",
    "    train_dataset = CustomDataset(train_messages, train_labels, tokenizer, MAX_LENGTH)\n",
    "    val_dataset = CustomDataset(val_messages, val_labels, tokenizer, MAX_LENGTH)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    NUM_LABELS = len(set(train_labels) | set(val_labels))\n",
    "    model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=NUM_LABELS)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    epoch_results = []\n",
    "    patience = 3\n",
    "    best_macro_f1 = 0\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "        metrics = eval_model(model, val_dataloader, criterion, num_labels=NUM_LABELS)\n",
    "        epoch_results.append(metrics)\n",
    "        print(metrics)\n",
    "\n",
    "        current_macro_f1 = metrics[\"macro_f1-score\"]\n",
    "        \n",
    "        if current_macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = current_macro_f1  # 更新最佳 macro_f1-score\n",
    "            epochs_without_improvement = 0   # 重置計數\n",
    "        else:\n",
    "            epochs_without_improvement += 1  # 增加計數\n",
    "        \n",
    "        # 若連續 patience 3 次沒有提升，則停止訓練\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    avg_metrics = {metric: np.mean([epoch[metric] for epoch in epoch_results]) for metric in epoch_results[0].keys()}\n",
    "    all_fold_results.append(avg_metrics)\n",
    "\n",
    "final_metrics = {metric: np.mean([result[metric] for result in all_fold_results]) for metric in all_fold_results[0].keys()}\n",
    "print(\"Final 10-fold Cross-Validation Results:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8016681-94ee-4d35-81ac-9831d7172f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.4774756056366813,\n",
       "  'val_accuracy': 0.6568880292284548,\n",
       "  'precision': 0.6401314716642911,\n",
       "  'recall': 0.6568880292284548,\n",
       "  'f1-score': 0.6322768268287018,\n",
       "  'macro_f1-score': 0.33303717513584924,\n",
       "  'balanced_accuracy': 0.34832429625173766,\n",
       "  'mcc': 0.5484266492879084},\n",
       " {'val_loss': 1.4332616797186701,\n",
       "  'val_accuracy': 0.6721470019342359,\n",
       "  'precision': 0.6488852121264229,\n",
       "  'recall': 0.6721470019342359,\n",
       "  'f1-score': 0.6414772608431984,\n",
       "  'macro_f1-score': 0.3016922162270063,\n",
       "  'balanced_accuracy': 0.30441537918176204,\n",
       "  'mcc': 0.5526329841903413},\n",
       " {'val_loss': 1.3589874485534938,\n",
       "  'val_accuracy': 0.6804570527974784,\n",
       "  'precision': 0.6606165277994105,\n",
       "  'recall': 0.6804570527974784,\n",
       "  'f1-score': 0.6545120360882776,\n",
       "  'macro_f1-score': 0.2908324130689872,\n",
       "  'balanced_accuracy': 0.3119277966285385,\n",
       "  'mcc': 0.5577025770043781},\n",
       " {'val_loss': 1.3847410363346273,\n",
       "  'val_accuracy': 0.6744155503020752,\n",
       "  'precision': 0.6551956633205985,\n",
       "  'recall': 0.6744155503020752,\n",
       "  'f1-score': 0.6464052563406941,\n",
       "  'macro_f1-score': 0.3074678667357438,\n",
       "  'balanced_accuracy': 0.30826172434245963,\n",
       "  'mcc': 0.5573892907333319},\n",
       " {'val_loss': 1.4502425631583542,\n",
       "  'val_accuracy': 0.6774302620456467,\n",
       "  'precision': 0.6558764084193387,\n",
       "  'recall': 0.6774302620456467,\n",
       "  'f1-score': 0.6456473691671156,\n",
       "  'macro_f1-score': 0.3372863537188115,\n",
       "  'balanced_accuracy': 0.3371887256441712,\n",
       "  'mcc': 0.5670162009618328},\n",
       " {'val_loss': 1.518370049569015,\n",
       "  'val_accuracy': 0.6505234410559855,\n",
       "  'precision': 0.6305156374556236,\n",
       "  'recall': 0.6505234410559855,\n",
       "  'f1-score': 0.623847315620477,\n",
       "  'macro_f1-score': 0.3129053128691258,\n",
       "  'balanced_accuracy': 0.33631892008475417,\n",
       "  'mcc': 0.5386450846307843},\n",
       " {'val_loss': 1.3690064467560008,\n",
       "  'val_accuracy': 0.68801775147929,\n",
       "  'precision': 0.6840172753917324,\n",
       "  'recall': 0.68801775147929,\n",
       "  'f1-score': 0.6685443595242416,\n",
       "  'macro_f1-score': 0.34608580073409356,\n",
       "  'balanced_accuracy': 0.37542780659858,\n",
       "  'mcc': 0.5696660733795748},\n",
       " {'val_loss': 1.4985221291590312,\n",
       "  'val_accuracy': 0.6657342657342658,\n",
       "  'precision': 0.6414368877859355,\n",
       "  'recall': 0.6657342657342658,\n",
       "  'f1-score': 0.6392565385908181,\n",
       "  'macro_f1-score': 0.31716401686727264,\n",
       "  'balanced_accuracy': 0.3425578699271427,\n",
       "  'mcc': 0.5542457171349677},\n",
       " {'val_loss': 1.4797614750896664,\n",
       "  'val_accuracy': 0.6689771766694843,\n",
       "  'precision': 0.6428930367149573,\n",
       "  'recall': 0.6689771766694843,\n",
       "  'f1-score': 0.6375179595374025,\n",
       "  'macro_f1-score': 0.36470848749415596,\n",
       "  'balanced_accuracy': 0.3757456006869756,\n",
       "  'mcc': 0.5543978220602988},\n",
       " {'val_loss': 1.4992300485664942,\n",
       "  'val_accuracy': 0.6465841850457235,\n",
       "  'precision': 0.6142281347526439,\n",
       "  'recall': 0.6465841850457235,\n",
       "  'f1-score': 0.6104918924916811,\n",
       "  'macro_f1-score': 0.3326314090730199,\n",
       "  'balanced_accuracy': 0.3476750683784945,\n",
       "  'mcc': 0.5445526961794269}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fold_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
